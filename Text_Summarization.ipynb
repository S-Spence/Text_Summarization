{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Summarization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrCnlYyenepI"
      },
      "source": [
        "The notebook below uses the nltk package (Natural Language Tool Kit) to create a summary of online articles. The sample article is a wikipedia article about reinforcement learning. Change the URL to get a summary of a different article. An article by Ekta Shah guided this approach to text summarization. \n",
        "https://www.analyticsvidhya.com/blog/2020/12/tired-of-reading-long-articles-text-summarization-will-make-your-task-easier/."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWWf-pz9VATV",
        "outputId": "40f24325-fc1c-4732-c580-7e75f0b70dc1"
      },
      "source": [
        "! pip install bs4\n",
        "! pip install lxml\n",
        "! pip install --user -U nltk\n",
        "\n",
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (4.2.6)\n",
            "Requirement already satisfied: nltk in /root/.local/lib/python3.7/site-packages (3.6.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /root/.local/lib/python3.7/site-packages (from nltk) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.62.3)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO9fM3kSYA5i"
      },
      "source": [
        "The code below obtains data through web scraping. The code uses the the BeautifulSoup and lxml libraries to parse text. Swap in another URL to summarize another article. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ggR_Z68XKmT"
      },
      "source": [
        "# Replace URL with another article\n",
        "scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Reinforcement_learning')\n",
        "article = scraped_data.read()\n",
        "parsed_article = bs.BeautifulSoup(article, 'lxml')\n",
        "paragraphs = parsed_article.find_all('p')\n",
        "article_text = \"\"\n",
        "for p in paragraphs:\n",
        "  article_text += p.text"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR0P7ip2YaAy"
      },
      "source": [
        "# Remove square brackets and extra spaces from article\n",
        "original_word_count = article_text.count(\" \") + 1\n",
        "article_text = re.sub(r\"[[0-9]*]\", \"\", article_text)\n",
        "article_text = re.sub(r\"\\s+\", \" \", article_text)\n",
        "\n",
        "# Remove special characters and extra whitespace\n",
        "formatted_text = re.sub(\"[^a-zA-Z]\", \" \", article_text)\n",
        "formatted_text = re.sub(r\"\\s+\", \" \", formatted_text)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfIeH6rigWZr"
      },
      "source": [
        "The code below creates a word frequency count. The nltk package provides stop words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WREb0x59di0m"
      },
      "source": [
        "# break sentences into words\n",
        "sentence_list = nltk.sent_tokenize(article_text)\n",
        "# obtain stop words from nltk library\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "word_frequencies = {}\n",
        "\n",
        "# Create a word count of all words that are not stopwords\n",
        "for word in nltk.word_tokenize(formatted_text):\n",
        "  if word not in stopwords:\n",
        "    if word not in word_frequencies.keys():\n",
        "      word_frequencies[word] = 1\n",
        "    else:\n",
        "      word_frequencies[word] +=1\n",
        "\n",
        "max_frequency = max(word_frequencies.values())\n",
        "\n",
        "# Calculate the weighted frequencies by dividing the frequency of each word by te max frequency \n",
        "for word in word_frequencies.keys():\n",
        "  word_frequencies[word] = (word_frequencies[word]/max_frequency)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1S2xaGhjYtK"
      },
      "source": [
        "Calculate scores for the sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_glX5KGhjXtq"
      },
      "source": [
        "sentence_scores = {}\n",
        "for sentence in sentence_list:\n",
        "  for word in nltk.word_tokenize(sentence.lower()):\n",
        "    if word in word_frequencies.keys() and len(sentence.split(' ')) < 30:\n",
        "        if sentence not in sentence_scores.keys():\n",
        "          sentence_scores[sentence] = word_frequencies[word]\n",
        "        else:\n",
        "          sentence_scores[sentence] += word_frequencies[word]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-CKwTLYpuVh"
      },
      "source": [
        "The code below creates a summary using the top n sentences in the sentence scores dictionary. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5-OgEO3lShN",
        "outputId": "32c69444-5e59-4983-f36e-c461d308e863"
      },
      "source": [
        "import heapq\n",
        "import textwrap\n",
        "\n",
        "# Create a summary of sentences using the top n sentences. \n",
        "summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
        "summary = \" \".join(summary_sentences)\n",
        "\n",
        "# Format paragraph output\n",
        "summary = textwrap.dedent(summary).strip()\n",
        "print(textwrap.fill(summary, width = 150))\n",
        "print(\"\")\n",
        "\n",
        "# Print orignal word count and summary word count\n",
        "word_count_summary = summary.count(\" \") + 1\n",
        "print(f\"Summary Word Count: {word_count_summary}\")\n",
        "print(f\"Original Word Count: {original_word_count}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning. Research topics\n",
            "include Associative reinforcement learning tasks combine facets of stochastic learning automata tasks and supervised learning pattern classification\n",
            "tasks. In reinforcement learning methods, expectations are approximated by averaging over samples and using function approximation techniques to cope\n",
            "with the need to represent value functions over large state-action spaces. The work on learning ATARI games by Google DeepMind increased attention to\n",
            "deep reinforcement learning or end-to-end reinforcement learning. Policy iteration consists of two steps: policy evaluation and policy improvement.\n",
            "Two elements make reinforcement learning powerful: the use of samples to optimize performance and the use of function approximation to deal with large\n",
            "environments. Assuming full knowledge of the MDP, the two basic approaches to compute the optimal action-value function are value iteration and policy\n",
            "iteration.\n",
            "\n",
            "Summary Word Count: 141\n",
            "Original Word Count: 3091\n"
          ]
        }
      ]
    }
  ]
}