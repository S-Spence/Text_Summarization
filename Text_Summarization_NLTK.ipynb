{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Summarization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrCnlYyenepI"
      },
      "source": [
        "<p>The notebook below uses the nltk package (Natural Language Tool Kit) to create a summary of online articles. The sample article is about text summarization using abstractive methods. Change the URL to get a summary of a different article. A <a href=\"https://www.analyticsvidhya.com/blog/2020/12/tired-of-reading-long-articles-text-summarization-will-make-your-task-easier/\">paper</a> by Ekta Shah guided this approach to text summarization. \n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWWf-pz9VATV",
        "outputId": "2502987d-d394-4a95-e1f4-7a53eb5c4b7f"
      },
      "source": [
        "! pip install bs4\n",
        "! pip install lxml\n",
        "! pip install --user -U nltk\n",
        "\n",
        "import bs4 as bs\n",
        "from urllib.request import Request, urlopen\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (4.2.6)\n",
            "Requirement already satisfied: nltk in /root/.local/lib/python3.7/site-packages (3.6.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.62.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /root/.local/lib/python3.7/site-packages (from nltk) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO9fM3kSYA5i"
      },
      "source": [
        "The code below obtains data through web scraping. The code uses the the BeautifulSoup and lxml libraries to parse text. Swap in another URL to summarize another article. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ggR_Z68XKmT"
      },
      "source": [
        "# Replace URL with another article\n",
        "# Work around 403 forbidden for web scrapping with bots\n",
        "req = Request('https://towardsdatascience.com/understanding-automatic-text-summarization-2-abstractive-methods-7099fa8656fe', headers={'User-Agent': 'Mozilla/5.0'})\n",
        "scraped_data = urlopen(req)\n",
        "\n",
        "article = scraped_data.read()\n",
        "parsed_article = bs.BeautifulSoup(article, 'lxml')\n",
        "paragraphs = parsed_article.find_all('p')\n",
        "article_text = \"\"\n",
        "for p in paragraphs:\n",
        "  article_text += p.text"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR0P7ip2YaAy"
      },
      "source": [
        "# Remove square brackets and extra spaces from article\n",
        "original_word_count = article_text.count(\" \") + 1\n",
        "article_text = re.sub(r\"[[0-9]*]\", \"\", article_text)\n",
        "article_text = re.sub(r\"\\s+\", \" \", article_text)\n",
        "\n",
        "# Remove special characters and extra whitespace\n",
        "formatted_text = re.sub(\"[^a-zA-Z]\", \" \", article_text)\n",
        "formatted_text = re.sub(r\"\\s+\", \" \", formatted_text)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfIeH6rigWZr"
      },
      "source": [
        "The code below creates a word frequency count. The nltk package provides stop words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WREb0x59di0m"
      },
      "source": [
        "# break sentences into words\n",
        "sentence_list = nltk.sent_tokenize(article_text)\n",
        "# obtain stop words from nltk library\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "word_frequencies = {}\n",
        "\n",
        "# Create a word count of all words that are not stopwords\n",
        "for word in nltk.word_tokenize(formatted_text):\n",
        "  if word not in stopwords:\n",
        "    if word not in word_frequencies.keys():\n",
        "      word_frequencies[word] = 1\n",
        "    else:\n",
        "      word_frequencies[word] +=1\n",
        "\n",
        "max_frequency = max(word_frequencies.values())\n",
        "\n",
        "# Calculate the weighted frequencies by dividing the frequency of each word by te max frequency \n",
        "for word in word_frequencies.keys():\n",
        "  word_frequencies[word] = (word_frequencies[word]/max_frequency)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1S2xaGhjYtK"
      },
      "source": [
        "Calculate scores for the sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_glX5KGhjXtq"
      },
      "source": [
        "sentence_scores = {}\n",
        "for sentence in sentence_list:\n",
        "  for word in nltk.word_tokenize(sentence.lower()):\n",
        "    if word in word_frequencies.keys() and len(sentence.split(' ')) < 30:\n",
        "        if sentence not in sentence_scores.keys():\n",
        "          sentence_scores[sentence] = word_frequencies[word]\n",
        "        else:\n",
        "          sentence_scores[sentence] += word_frequencies[word]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-CKwTLYpuVh"
      },
      "source": [
        "The code below creates a summary using the top n sentences in the sentence scores dictionary. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5-OgEO3lShN",
        "outputId": "fcd3a005-d17e-4357-c441-625db30fe4ec"
      },
      "source": [
        "import heapq\n",
        "import textwrap\n",
        "\n",
        "# Create a summary of sentences using the top n sentences. \n",
        "summary_sentences = heapq.nlargest(9, sentence_scores, key=sentence_scores.get)\n",
        "summary = \" \".join(summary_sentences)\n",
        "\n",
        "# Format paragraph output\n",
        "summary = textwrap.dedent(summary).strip()\n",
        "print(textwrap.fill(summary, width = 100))\n",
        "print(\"\")\n",
        "\n",
        "# Print orignal word count and summary word count\n",
        "word_count_summary = summary.count(\" \") + 1\n",
        "print(f\"Summary Word Count: {word_count_summary}\")\n",
        "print(f\"Original Word Count: {original_word_count}\")\n",
        "\n",
        "# Uncomment to store data in a text file\n",
        "#!echo \"Text Summarization Techniques\\n\" > text_summarization_research.txt\n",
        "#with open('text_summarization_research.txt', 'a') as writefile:\n",
        "#    writefile.write(textwrap.fill(summary, width=150))\n",
        "#    writefile.write(\"\\n\")\n",
        "#    writefile.write(f\"Summary word count: {word_count_summary}\\n\")\n",
        "#    writefile.write(f\"Original word count: {original_word_count}\\n\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The word embeddings help to gain several insights about the word like whether a given word is\n",
            "similar to a given word or not. The decoder model takes in the inputs and generates the predicted\n",
            "words of the output sequence, given the previous word generated. The term “sequence to sequence\n",
            "models” is used because the models are designed to create an output sequence of words from an input\n",
            "sequence of words. So, the decoder has basically two actions at a time step, it can generate a word\n",
            "from the target dictionary or it can point and copy a word. The decoder generates the words time\n",
            "steps by time steps until the <end> tag is faced.This might raise a question, how are the words\n",
            "generated?. We can see the for predicting a word each word in the input sequence is being assigned a\n",
            "weight, called the attention weights. So, the paper proposed to take into consideration factors like\n",
            "part of speech tags, named-entity tags, and TFIDF statistics of a word alongside embeddings to\n",
            "represent a word. This turns off the switch and a word is copied from the input text.This is an\n",
            "overview of the sequence-to-sequence model for text summarization. Alongside, the models, usually\n",
            "use word embeddings of the words in the vocabulary from well-known embedding vectors like the\n",
            "word2vec by google or the Glove by Standford NLP.\n",
            "\n",
            "Summary Word Count: 227\n",
            "Original Word Count: 2542\n"
          ]
        }
      ]
    }
  ]
}